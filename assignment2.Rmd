```{r}
load("proj_data.Rdata")
modell <- glm(Resultat ~ Alder + Kon + Utbildare, 
              data = data_individ,
              family = "binomial")
summary(modell)
```
```{r}
source("funktioner.R")
y <- matrix(data_individ$Resultat, ncol = 1)
X <- model.matrix(Resultat ~ Alder + Kon + Utbildare, 
                  data = data_individ)
```

## Task 1:

Verify, using functions I and NR from Part I, that the z value column of the output are Wald statistics (see the textbook page 128).
Z value column? Which is that? the output?
```{r}

## Are these the z values we are supposed to work with?
z_values <- unname(summary(modell)$coefficients[, "Estimate"]) #THESE ARE NOT Z VALUES DO NOT TRUST

```

```{r}
  theta0 <- c(0, 0, 0, 0)
  NR_estimate <- NR(theta0, 5, y, X)
  I_estimate <-I(NR_estimate, y, X) # Is NR the z that we hear about?
```
```{r}
  # According to the book 
  
  #true_wald_statistics <-%*%(NR_estimate-theta0) # Walder fixed: It was just the inverse
  true_wald_statistics <- (NR_estimate- theta0)/(sqrt(diag(solve(I(NR_estimate, y, X)))))
  print(true_wald_statistics)
```
## Task 2:

Compute the generalized likelihood ratio statistics (see textbook chapter 5.5) that corresponds to the Wald statistics in Task 1 and determine the corresponding ð‘ƒ
-values. Note that your likelihood ratio statistics should be of the same order of magnitude as the squared Wald statistics (why?).
```{r}
  true_wald_statistics
```
```{r}
  theta1 <- c(0, 0, 0)
  for (i in 1:length(eta)){
      NR_est <- NR(theta0 = theta1, niter = 10, y = y, X = X[, -i])
      print(NR_est)
    }
```
```{r}
  theta1 <- c(0, 0, 0)
  eta <- NR(c(0, 0, 0), niter = 10, y = y, X = X[, -3])
  null <- l(eta, y, X[-3])
  alt_eta <- NR(c(0, 0, 0, 0), niter = 10, y = y, X)
  alt <- l(alt_eta, y, X)
  gl <- 2*(null-alt)
```

```{r}
  theta1 <- c(0, 0, 0)
  eta <- NR(theta0 = theta1, niter = 10, y = y, X = X[, -3])
  #print(eta)
  gLikelihood <- 2*log(L(eta, y, X[, -3])/L(theta1, y, X[, -3]))
  #print(gLikelihood)
  for (i in 1:length(eta)){
    #print(i)
    #gLikelihood <- 2*log(L(eta[i], y, X[, -i])/L(theta1[i], y, X[, -i]))
    theta_i <- eta
    theta_i[-i] <- theta1[-i]
    print(theta_i)
    gLikelihood <- 2*(l(theta_i, y, X[, -i])- l(theta1, y, X[, -i]))
    print(gLikelihood)
  }
  #gLikelihood <- 2*log(L(eta, y, X[, -3])/L(theta1, y, X[, -3]))
  #gLikelihood
  
```
```{r}

## Iterating through X's columns, comparing and obtaining the ratio 
  x_name = c('intercept', 'alder', 'kon', 'utbildare')
  results <- matrix(NA, nrow = length(x_name), ncol = 3)
  
  colnames(results) <- c("x_name", "gLikelihood", "p_value")
  eta <- NR(theta0 = theta1, niter = 10, y = y, X = X[, -3])
  Lp_null <-L(NR_estimate, y, X)
  
  for (i in 1:(length(eta)+1)) {
    #print(x_name[i])
    eta <- NR(c(0,0,0), niter = 10, y = y, X = X[, -i])
    Lp_ML <- L(eta, y, X[,-i])
    gLikelihood <-  2*(log(Lp_null) - log(Lp_ML))
    #print(W)
    p_value <- pchisq(gLikelihood, 1, lower.tail = FALSE)
    #print(p_value)
    results[i, ] <- c(x_name[i], gLikelihood,p_value)
  }
  
  print(results)
```
Task 3:

The score statistic can, like the likelihood ratio statistic, be generalized to the case with a nuisance parameter ðœ‚. The generalized score statistic is

ð‘‡ð‘†(ðœ½0)=ð‘†(ðœ½0,ðœ¼Ì‚ ð‘€ð¿(ðœ½0))ð‘‡ð¼(ðœ½0,ðœ¼Ì‚ ð‘€ð¿(ðœ½0))âˆ’1ð‘†(ðœ½0,ðœ¼Ì‚ ð‘€ð¿(ðœ½0))
with an asymptotic ðœ’2(ð‘ž) distribution (notation following the textbook chapter 5.5). An advantage of this statistic is thet the ML-estimate only needs to be computed under the null hypothesis. Compute the ML estimate of ðœ¼=(ðœƒð´ð‘™ð‘‘ð‘’ð‘Ÿ,ðœƒð‘ˆð‘¡ð‘ð‘–ð‘™ð‘‘ð‘Žð‘Ÿð‘’) under ð»0:ðœ½=(ðœƒð‘–ð‘›ð‘¡ð‘’ð‘Ÿð‘ð‘’ð‘ð‘¡,ðœƒð¾ð‘œð‘›)=(0,0) and use this to determine a ð‘ƒ
-value based on the generalized score statistic (a model without intercept is somewhat weird for this case, so the intercept should be included regardless of its significance).
If we want to maximize ðœ¼â†¦ð¿(ðœ½,ðœ¼) for a fix ðœ½â‰ 0 the function NR needs to be modified. Instead of doing so, we use Râ€™s glm function with a so called offset. An offset is a variable ð‘œð‘– that is added to the linear component ð‘¥ð‘–ðœƒwithout a coefficient. For the logistic regression with offset ð‘œð‘–we then get ð‘(ð‘¥ð‘–)=(1+exp(âˆ’ð‘¥ð‘–ðœƒ+ð‘œð‘–))

