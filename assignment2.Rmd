---
title: "Assignment1"
author: "Regina Crespo Lopez Oliver (20000322-T884) & Malin Mueller (20011115-T460)"
date: "2024-09-27"
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
  # Byt √Ö√ÖMMDD mot ditt f√∂delsedatum 
  set.seed(011115) # - Malin
  # set.seed(000322) # - Regina
```


```{r}
load("proj_data.Rdata")
modell <- glm(Resultat ~ Alder + Kon + Utbildare, 
              data = data_individ,
              family = "binomial")
summary(modell)
```
```{r}
  source("funktioner.R")
  y <- matrix(data_individ$Resultat, ncol = 1)
  X <- model.matrix(Resultat ~ Alder + Kon + Utbildare, 
                    data = data_individ)
```

## Task 1:

Verify, using functions I and NR from Part I, that the z value column of the output are Wald statistics (see the textbook page 128).
Z value column? Which is that? the output?

```{r}
  theta0 <- c(0, 0, 0, 0)
  NR_estimate <- NR(theta0, 5, y, X)
  I_estimate <-I(NR_estimate, y, X) # Is NR the z that we hear about?
```
```{r}
  # According to the book 
  
  #true_wald_statistics <-%*%(NR_estimate-theta0) # Walder fixed: It was just the inverse
  true_wald_statistics <- (NR_estimate- theta0)/(sqrt(diag(solve(I(NR_estimate, y, X)))))
  inv_diag <- diag(solve(I(NR_estimate, y, X)))
  print(true_wald_statistics)
```
## Task 2:

Compute the generalized likelihood ratio statistics (see textbook chapter 5.5) that corresponds to the Wald statistics in Task 1 and determine the corresponding ùëÉ
-values. Note that your likelihood ratio statistics should be of the same order of magnitude as the squared Wald statistics (why?).


```{r}
  theta1 <- c(0, 0, 0)
## Iterating through X's columns, comparing and obtaining the ratio 
  x_name = c('intercept', 'alder', 'kon', 'utbildare')
  results <- matrix(NA, nrow = length(x_name), ncol = 3)
  
  colnames(results) <- c("x_name", "gLikelihood", "p_value")
  Lp_null <-L(NR_estimate, y, X)
  
  for (i in 1:(length(eta)+1)) {
    #print(x_name[i])
    eta <- NR(theta1, niter = 10, y = y, X = X[, -i])
    Lp_ML <- L(eta, y, X[,-i])
    gLikelihood <-  2*(log(Lp_null) - log(Lp_ML))
    #print(W)
    p_value <- pchisq(gLikelihood, 1, lower.tail = FALSE)
    #print(p_value)
    results[i, ] <- c(x_name[i], gLikelihood,p_value)
  }
  
  print(results)
```
Comparing the results of the generalized ratio statistics (above), with the Wald statistics (below),
shows the expected outcome of the task. The generalized likelihood statistics is in the same
order of magnitude as the wald statistics, and even displays similar values.

```{r}
  true_wald_statistics^2
```
# Task 3
The score statistic can, like the likelihood ratio statistic, be generalized to the case with a nuisance parameter ùúÇ. The generalized score statistic is

ùëáùëÜ(ùúΩ0)=ùëÜ(ùúΩ0,ùúºÃÇ ùëÄùêø(ùúΩ0))ùëáùêº(ùúΩ0,ùúºÃÇ ùëÄùêø(ùúΩ0))‚àí1ùëÜ(ùúΩ0,ùúºÃÇ ùëÄùêø(ùúΩ0))
with an asymptotic ùúí2(ùëû) distribution (notation following the textbook chapter 5.5). An advantage of this statistic is thet the ML-estimate only needs to be computed under the null hypothesis. Compute the ML estimate of ùúº=(ùúÉùê¥ùëôùëëùëíùëü,ùúÉùëàùë°ùëèùëñùëôùëëùëéùëüùëí) under ùêª0:ùúΩ=(ùúÉùëñùëõùë°ùëíùëüùëêùëíùëùùë°,ùúÉùêæùëúùëõ)=(0,0) and use this to determine a ùëÉ
-value based on the generalized score statistic (a model without intercept is somewhat weird for this case, so the intercept should be included regardless of its significance).
If we want to maximize ùúº‚Ü¶ùêø(ùúΩ,ùúº) for a fix ùúΩ‚â†0 the function NR needs to be modified. Instead of doing so, we use R‚Äôs glm function with a so called offset. An offset is a variable ùëúùëñ that is added to the linear component ùë•ùëñùúÉwithout a coefficient. For the logistic regression with offset ùëúùëñwe then get ùëù(ùë•ùëñ)=(1+exp(‚àíùë•ùëñùúÉ+ùëúùëñ))

```{r}
  #walder and utbilder are columns 2 and 4 - so we exclude them 
  #from restricted X - we only fit model under H0
  eta <- NR(c(0,0), niter = 10, y = y, X = X[, c(-2,-4)]) 
  #compute MLE for nuisance intercept and kon
  print(eta)
  
  #scover, fisher and t at mle for incercept and kon, and null values for alder and utbildare
  score <- S(eta, y, X[, c(-2,-4)])
  print(score)
  fisher <- I(eta, y, X[, c(-2,-4)])
```


```{r}
  print(fisher)
  T <- t(score)%*%solve(fisher)%*%score
  print(T)
  
  p_val_T <- pchisq(T, 1, lower.tail = FALSE)
  print(p_val_T)
```
#Task 4:
Compute the profile likelihood (textbook definition 5.4) for parameter ùúÉùêæùëúùëõ , ùêøùëù(ùúÉùêæùëúùëõ), on a suitable grid of parameter
values. Use these to graph ùêøùëù together with the corresponding estimated likelihood (textbook definition 5.5). In order to 
determine ùúÇÃÇ ùëÄùêø(ùúÉùêæùëúùëõ) you may for example use the glm.fit function with an extra offset as in which gives estimates of the other coefficients when ùúÉùêæùëúùëõ=0.5
 (as an example value). Decide a 95% confidence interval based on the profile likelihood visually from the figure by drawing a horizontal line at a suitable level (c.f. Figure 5.3b in the textbook). The choice of level should be motivated and the result compared with the corresponding Wald interval.t
```{r}
theta.Kon <- 0.5 # example value
profil <- glm.fit(x = X[, -3], y = y,
                  offset = theta.Kon * X[, 3],
                  family = binomial())
profil$coeff
```


 
```{r}
## Estimated likelihood
  L(theta.Kon, y, X[,-3])

```
```{r}
# likelihood profile for L(theta, eta)
  compute_profile_likelihood <- function(theta_kon, y, X){
    
    #set theta
    theta.Kon <- theta_kon
    #fit logistic regression model
    profile <- glm.fit(x = X[, -3], y = y,
                      offset = theta.Kon * X[, 3],
                      family = binomial())
    # extract MLEs for intercep, age, education
    eta_ML <- profile$coeff
    
    #create a vector with all of our parameter values
    full_params <- c(eta_ML[1], eta_ML[2], theta_kon, eta_ML[3])
    profile_L <- L(full_params, y, X)
    return(profile_L)
  }

  # 100 values of theta.kon from -1 to 1
  theta_grid <- seq(-1, 1, length.out = 100)  # Values of theta_Kon from -1 to 1
  
  #profile likelihood
  profile_likelihood_values <- sapply(theta_grid, FUN = function(theta_kon) compute_profile_likelihood(theta_kon, y, X))
  
  # normalize profile likelihood
  normalized_profile_likelihood <- (profile_likelihood_values - min(profile_likelihood_values)) / (max(profile_likelihood_values) - min(profile_likelihood_values))
  
  
  #estimated likelihood of full model L(theta_kon)
  estimated_likelihood_values <- sapply(theta_grid, function(theta) {
    L(c(NR_estimate[1], NR_estimate[2], theta, NR_estimate[4]), y, X)
  })
  
  normalized_estimated_likelihood <- (estimated_likelihood_values - min(estimated_likelihood_values)) / (max(estimated_likelihood_values) - min(estimated_likelihood_values))
  
  
  #plot
  plot(theta_grid, normalized_profile_likelihood ,ylab="normalized profile Likelihoods",xlab="theta_KonMan",type="l", col="red", ylim=c(0,1))
  lines(theta_grid, normalized_estimated_likelihood, col = "blue", lwd = 2)
  abline(h=0.147) #  The value corresponds to the likelihood threshold for a 95% confidence interval 
  legend(-1,1, legend=c("Profile Likelihood", "Estimated Likelihood", "Confidence Level"), col= c("red", "blue", "black"), lty=1, cex=0.8)

```
Where the likelihood crosses horizontal black line gives you a 95% CI for theta kon
```{r}
 # Wald confidence interval
  wald_ci_lower <-  NR_estimate[3] - 1.96 * inv_diag[3]
  wald_ci_upper <-  NR_estimate[3] + 1.96 * inv_diag[3]
  print(wald_ci_lower)
  print(wald_ci_upper)
```

 
 
 